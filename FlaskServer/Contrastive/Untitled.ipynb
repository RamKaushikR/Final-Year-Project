{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import librosa\n",
    "import numpy as np\n",
    "from kapre.composed import get_melspectrogram_layer\n",
    "from tensorflow.keras import backend as K\n",
    "import speech_recognition as sr\n",
    "\n",
    "\n",
    "SPEAKERS_PATH = 'Speakers/'\n",
    "PREDICT_PATH = 'Predict/'\n",
    "SR = 16000\n",
    "TIME = 3 * SR\n",
    "AUDIO_SHAPE = (TIME, 1)\n",
    "SPEC_SHAPE = (300, 128, 1)\n",
    "\n",
    "def spectrogram(audio):\n",
    "\tlayer = get_melspectrogram_layer(input_shape=AUDIO_SHAPE,\n",
    "                                     n_mels=128, pad_end=True,\n",
    "                                     n_fft=512, win_length=400,\n",
    "                                     hop_length=160, sample_rate=SR,\n",
    "                                     return_decibel=True,\n",
    "                                     input_data_format='channels_last',\n",
    "                                     output_data_format='channels_last'\n",
    "                                    )\n",
    "                                    \n",
    "\taudio = np.expand_dims(audio, axis=1)\n",
    "\taudio = np.reshape(audio, (1, audio.shape[0], audio.shape[1]))\n",
    "\tspec = layer(audio)\n",
    "\treturn spec\n",
    "\t\n",
    "def speechToText(recognizer, path, audio_text):\n",
    "\ta = sr.AudioFile(path)\n",
    "\ttext = ''\n",
    "\t\n",
    "\twith a as source:\n",
    "\t\taudio = recognizer.record(source)\n",
    "\t\ttext = recognizer.recognize_google(audio)\n",
    "\t\t\n",
    "\ttext = text.lower()\n",
    "\taudio_text = audio_text.lower()\n",
    "\tprint('Received text is: ' + audio_text)\n",
    "\tprint('Speech to text is: ' + text)\n",
    "\t\n",
    "\tpred_dict, audio_dict = {}, {}\n",
    "\t\n",
    "\tfor word in list(text.split(' ')):\n",
    "\t\tpred_dict[word] = pred_dict[word] + 1 if word in pred_dict else 1\n",
    "\tfor word in list(audio_text.split(' ')):\n",
    "\t\taudio_dict[word] = audio_dict[word] + 1 if word in audio_dict else 1\n",
    "\t\t\n",
    "\tcount = 0\n",
    "\tfor word in pred_dict:\n",
    "\t\tif word in audio_dict:\n",
    "\t\t\tcount += min(audio_dict[word], pred_dict[word])\n",
    "\t\t\t\t\n",
    "\treturn count / sum(audio_dict.values())\n",
    "\n",
    "def add(speaker, recognizer, audio_text, dependent):\n",
    "\tif speaker + '.dat' in os.listdir(SPEAKERS_PATH):\n",
    "\t\tos.remove(SPEAKERS_PATH+speaker+'.wav')\n",
    "\t\treturn speaker + ' has already been enrolled'\n",
    "\n",
    "\tif dependent == 'True':\n",
    "\t\ttext = speechToText(recognizer, SPEAKERS_PATH+speaker+'.wav', audio_text)\n",
    "\t\tif text < 0.70:\n",
    "\t\t\tos.remove(SPEAKERS_PATH+speaker+'.wav')\n",
    "\t\t\treturn 'Text match unsuccessful'\n",
    "\n",
    "\ta, r = librosa.load(SPEAKERS_PATH+speaker+'.wav')\n",
    "\ta = librosa.resample(a, r, SR)\n",
    "\tif a.shape[0] < TIME:\n",
    "\t\tos.remove(SPEAKERS_PATH+speaker+'.wav')\n",
    "\t\treturn 'Invalid File. Talk a little slower'\n",
    "\t\t\n",
    "\ti = (a.shape[0] - TIME) // 2#np.random.randint(0, a.shape[0] - TIME)\n",
    "\taudio = a[i:i+TIME]\n",
    "\t\n",
    "\tspec = abs(spectrogram(audio))\n",
    "\tspec = np.reshape(spec, (spec.shape[1], spec.shape[2]))\n",
    "\t\n",
    "\twith open(SPEAKERS_PATH+speaker+'.dat', 'wb') as f:\n",
    "\t\tpickle.dump(spec, f)\n",
    "\tos.remove(SPEAKERS_PATH+speaker+'.wav')\n",
    "\t\n",
    "\treturn speaker + ' added successfully'\n",
    "\t\n",
    "def remove(speaker):\n",
    "\tif speaker + '.dat' in os.listdir(SPEAKERS_PATH):\n",
    "\t\tos.remove(SPEAKERS_PATH+speaker+'.dat')\n",
    "\t\treturn speaker + ' removed successfully'\n",
    "\t\t\n",
    "\treturn speaker + ' has not been enrolled'\n",
    "\t\n",
    "def euclideanDistance(inputs):\n",
    "\tu, v = inputs\n",
    "\treturn K.sqrt(K.sum(K.square(u - v), axis=1, keepdims=True))\n",
    "\t\n",
    "def predict(audio_text, model, threshold, recognizer, dependent):\n",
    "\tresult = {'text': '', 'speaker': ''}\n",
    "\ts = os.listdir(SPEAKERS_PATH)\n",
    "\tp = os.listdir(PREDICT_PATH)\n",
    "\t\n",
    "\tif dependent == 'True':\n",
    "\t\ttext = speechToText(recognizer, PREDICT_PATH+p[0], audio_text)\n",
    "\t\tif text < 0.75:\n",
    "\t\t\tresult['text'] = 'Text match unsuccessful'\n",
    "\t\telse:\n",
    "\t\t\tresult['text'] = 'Text match successful'\n",
    "\n",
    "\tspeakers = []\n",
    "\tfor _ in s:\n",
    "\t\tspec = pickle.load(open(SPEAKERS_PATH+_, 'rb'))\n",
    "\t\tspec = np.reshape(spec, SPEC_SHAPE)\n",
    "\t\tspeakers.append(abs(spec))\n",
    "\tspeakers = np.array(speakers)\n",
    "\t\n",
    "\ta, r = librosa.load(PREDICT_PATH+p[0])\n",
    "\ta = librosa.resample(a, r, SR)\n",
    "\tif a.shape[0] < TIME:\n",
    "\t\tresult['text'] = 'Invalid File. Talk a little slower'\n",
    "\t\tresult['speaker'] = ''\n",
    "\t\treturn result\n",
    "\t\t\n",
    "\ti = (a.shape[0] - TIME) // 2#np.random.randint(0, a.shape[0] - TIME)\n",
    "\taudio = a[i:i+TIME]\n",
    "\t\n",
    "\tpredict_spec = abs(spectrogram(audio))\n",
    "\tpredict_spec = np.reshape(predict_spec, (1, 300, 128, 1))\n",
    "\t\n",
    "\ty_true = model.predict(speakers)\n",
    "\ty_pred = model.predict(predict_spec)\n",
    "    \n",
    "\tdist = euclideanDistance((y_true, y_pred))\n",
    "\tindex = np.argmin(dist)\n",
    "\tprint(dist, s)\n",
    "\t\n",
    "\tif dist[index] >= threshold:\n",
    "\t\tresult['speaker'] = 'Speaker Not Found'\n",
    "\telse:\n",
    "\t\tresult['speaker'] = s[index][:-4] + ' is the identified speaker'\n",
    "\t\t\n",
    "\treturn result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
